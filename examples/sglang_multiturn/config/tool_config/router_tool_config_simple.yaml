tools:
- class_name: verl.tools.router_tool.SelectTool
  config: {}
  tool_schema:
    type: function
    function:
      name: select_response
      description: 'Select the best response from previously executed model calls
        to provide as the final answer.


        Workflow:

        1. First, call one or more model tools (e.g., call_gpt_4o, call_deepseek_r1)
        to generate responses

        2. Compare and evaluate the quality of different model responses

        3. Use this tool to select the most appropriate response as your final answer


        Requirements:

        - Only call this tool AFTER executing other model tools

        - The model_call_name must match exactly with a previously called function
        name

        - Function names always start with ''call_''.


        The selected response will be returned to the user as the definitive answer
        to their question.'
      parameters:
        type: object
        properties:
          model_call_name:
            type: string
            description: Exact name of the previously called model function whose
              response you want to select. Must start with 'call_' and match a function
              that was already executed (e.g., 'call_gpt_4o', 'call_deepseek_r1').
        required:
        - model_call_name
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_5
      description: "Call GPT-5 model. Latest flagship model with built-in reasoning\
        \ capabilities and expert-level intelligence.\n\n        Capabilities: Advanced\
        \ reasoning with invisible thinking, coding, math, general tasks, multimodal\
        \ (text+image input), long context, enhanced tool calling\n        Quality:\
        \ Premium tier with state-of-the-art performance\n        Cost: $1.25/M input\
        \ tokens, $10.00/M output tokens\n        Context: 400K tokens\n        Max\
        \ Output: 128K tokens (includes reasoning tokens)"
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_5_mini
      description: "Call GPT-5 Mini model. Smaller, faster, and more cost-effective\
        \ version of GPT-5 with strong performance.\n\n        Capabilities: Reasoning,\
        \ coding, math, general tasks, multimodal (text+image input)\n        Quality:\
        \ Mid-tier with excellent performance-to-cost ratio\n        Cost: $0.25/M\
        \ input tokens, $2.00/M output tokens\n        Context: 400K tokens\n    \
        \    Max Output: 128K tokens"
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_5_nano
      description: "Call GPT-5 Nano model. Ultra-efficient and cost-effective GPT-5\
        \ variant for high-volume applications.\n\n        Capabilities: Basic reasoning,\
        \ coding, math, general tasks\n        Quality: Budget tier with optimized\
        \ efficiency\n        Cost: $0.05/M input tokens, $0.40/M output tokens\n\
        \        Context: 400K tokens\n        Max Output: 128K tokens"
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_4o
      description: "Call GPT-4o (Omni) model. Advanced multimodal model with excellent\
        \ reasoning and coding capabilities.\n        \n        Capabilities: Advanced\
        \ reasoning, coding, math, general tasks, multimodal\n        Quality: Premium\
        \ tier  \n        Cost: $2.50/M input tokens, $10.00/M output tokens\n   \
        \     Context: 128K tokens\n        Max Output: 16,384 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_4o_mini
      description: "Call GPT-4o Mini model. Cost-effective model with a strong balance\
        \ of performance, speed, and affordability.\n        \n        Capabilities:\
        \ Reasoning, coding, math, general tasks\n        Quality: Budget tier with\
        \ good performance\n        Cost: $0.15/M input tokens, $0.60/M output tokens\n\
        \        Context: 128K tokens\n        Max Output: 16,384 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_4_1
      description: "Call GPT-4.1 model. Latest GPT-4 variant with a massive context\
        \ window and state-of-the-art coding and instruction-following abilities.\n\
        \        \n        Capabilities: Advanced reasoning, coding, math, general\
        \ tasks, long context processing\n        Quality: Premium tier\n        Cost:\
        \ $2.00/M input tokens, $8.00/M output tokens\n        Context: 1,047,576\
        \ tokens (~1M tokens)\n        Max Output: 32,768 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_4_1_mini
      description: "Call GPT-4.1 Mini model. Balanced model with a large context window\
        \ and an exceptional performance-to-cost ratio.\n        \n        Capabilities:\
        \ Reasoning, coding, math, general tasks, long context processing\n      \
        \  Quality: Budget tier\n        Cost: $0.40/M input tokens, $1.60/M output\
        \ tokens\n        Context: 1,047,576 tokens (~1M tokens)\n        Max Output:\
        \ 32,768 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_4_1_nano
      description: "Call GPT-4.1 Nano model. Ultra-low cost model with a large context\
        \ window, optimized for speed in high-volume tasks.\n        \n        Capabilities:\
        \ Basic reasoning, coding, math, general tasks, long context processing\n\
        \        Quality: Budget tier\n        Cost: $0.10/M input tokens, $1.40/M\
        \ output tokens\n        Context: 1,047,576 tokens (~1M tokens)\n        Max\
        \ Output: 32,768 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_o3
      description: "Call o3 model. Advanced reasoning model optimized for mathematical\
        \ and scientific tasks, trained to \"think\" before answering.\n        \n\
        \        Capabilities: Advanced reasoning, mathematics, scientific analysis\n\
        \        Quality: Specialized for reasoning tasks\n        Cost: $2.00/M input\
        \ tokens, $8.00/M output tokens, its reasoning tokens could be very long,\
        \ so the cost could be very high\n        Context: 200K tokens\n        Max\
        \ Output: 100,000 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_o3_pro
      description: "Call o3 Pro model. Premium reasoning model for the most challenging\
        \ problems, using more compute to \"think harder\".\n        \n        Capabilities:\
        \ Premium reasoning, advanced mathematics, scientific research\n        Quality:\
        \ Top-tier specialized model\n        Cost: $20.00/M input tokens, $80.00/M\
        \ output tokens, its reasoning tokens could be very long, so the cost could\
        \ be very high\n        Context: 200K tokens\n        Max Output: 100,000\
        \ tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_o4_mini
      description: "Call o4 Mini model. High-performance reasoning model with an excellent\
        \ performance-to-cost ratio.\n        \n        Capabilities: Reasoning, mathematics,\
        \ scientific analysis\n        Quality: Standard tier for reasoning\n    \
        \    Cost: $1.10/M input tokens, $4.40/M output tokens, its reasoning tokens\
        \ could be very long, so the cost could be very high\n        Context: 200K\
        \ tokens\n        Max Output: 100,000 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_oss_120b
      description: "Call GPT-OSS 120B model. Open-weight MoE model from OpenAI, designed\
        \ for efficient on-premise deployment and strong reasoning.\n        \n  \
        \      Capabilities: Reasoning, coding, mathematics, general tasks, chain-of-thought\n\
        \        Quality: Standard tier.\n        Cost: $0.15/M input tokens, $0.60/M\
        \ output tokens\n        Context: 131,072 tokens (128K)\n        Max Output:\
        \ 131,072 tokens\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gpt_oss_20b
      description: "Call GPT-OSS 20B model. Open-weight MoE model from OpenAI, designed\
        \ for highly efficient on-premise deployment and strong reasoning.\n     \
        \   \n        Capabilities: Reasoning, mathematics, general tasks, chain-of-thought\n\
        \        Quality: Budget tier.\n        Cost: $0.05/M input tokens, $0.20/M\
        \ output tokens\n        Context: 131,072 tokens (128K)\n        Max Output:\
        \ 131,072 tokens"
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_qwen3_235b_instruct
      description: "Call Qwen3 235B Instruct model. Large-scale, open-weight MoE model\
        \ with excellent multilingual capabilities and strong all-around performance.\n\
        \        \n        Capabilities: Reasoning, coding, math, general tasks, multilingual\n\
        \        Quality: Premium tier with excellent multilingual support\n     \
        \   Cost: $0.20/M input tokens, $0.60/M output tokens\n        Context: 262,144\
        \ tokens\n        Max Output: 262,144 tokens\n        \n        Best for:\
        \ High-quality, cost-effective inference for multilingual applications and\
        \ general-purpose tasks requiring strong instruction-following.\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_qwen3_235b_thinking
      description: "Call Qwen3 235B Thinking model. Specialized open-weight reasoning\
        \ model with built-in, transparent chain-of-thought capabilities.\n      \
        \  \n        Capabilities: Advanced reasoning, mathematics, analysis, chain-of-thought\n\
        \        Quality: Specialized for reasoning with built-in CoT\n        Cost:\
        \ $0.65/M input tokens, $3.00/M output tokens, its reasoning tokens could\
        \ be very long, so the cost could be very high\n        Context: 262,144 tokens\n\
        \        Max Output: 262,144 tokens\n        \n        Best for: Complex analytical\
        \ problems, mathematical proofs, and logical analysis requiring transparent,\
        \ step-by-step reasoning chains.\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_qwen3_coder_480b
      description: "Call Qwen3 Coder 480B model. Massive 480B parameter, open-weight\
        \ MoE model hyper-specialized for coding.\n        \n        Capabilities:\
        \ Advanced coding, debugging, code review, programming\n        Quality: Specialized\
        \ coding model.\n        Cost: $2.00/M input tokens, $2.00/M output tokens\n\
        \        Context: 262,144 tokens\n        Max Output: 262,144 tokens\n   \
        \     \n        Best for: The most demanding, end-to-end software engineering\
        \ tasks, including complex code generation, large-scale refactoring, and agentic\
        \ development workflows.\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_kimi_k2
      description: "Call Kimi K2 Instruct model. Advanced 1T parameter open-weight\
        \ MoE model, uniquely trained for agentic workflows and tool use.\n      \
        \  \n        Capabilities: Advanced reasoning, mathematics, analysis, multilingual,\
        \ agentic coding\n        Quality: Premium tier with strong analytical and\
        \ tool-use capabilities\n        Cost: $1.00/M input tokens, $3.00/M output\
        \ tokens\n        Context: 131,072 tokens\n        Max Output: 131,072 tokens\n\
        \        \n        Best for: Agentic coding workflows requiring interaction\
        \ with external tools, complex analytical tasks, and multilingual applications.\n\
        \        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_deepseek_r1
      description: "Call DeepSeek R1 model. Research-focused strong reasoning model.\n\
        \n        Capabilities: Reasoning, mathematics, science, research\n      \
        \  Quality: Premium tier with strong research capabilities\n        Cost:\
        \ $3.00/M input tokens, $7.00/M output tokens\n        Context: 163,840 tokens\n\
        \        Max Output: 131,072 tokens\n\n        Best for: Creative Writing,scientific\
        \ reasoning and analytical problem solving."
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_deepseek_r1_tput
      description: "Call DeepSeek R1 Throughput model. High-throughput version of\
        \ DeepSeek R1 with optimized cost.\n        \n        Capabilities: Reasoning,\
        \ mathematics, science with optimized speed\n        Quality: Standard tier\
        \ with better cost-performance\n        Cost: $0.55/M input tokens, $2.19/M\
        \ output tokens\n        Context: 163,840 tokens\n        Max Output: 131,072\
        \ tokens\n        \n        Best for: Scaling reasoning tasks that require\
        \ good performance at a lower cost, particularly for mathematical problems\
        \ where speed and cost are key factors.\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gemini_2_5_pro
      description: "Call Gemini 2.5 Pro model. Google's most advanced model with elite\
        \ reasoning, controllable \"thinking\" capabilities, and a massive context\
        \ window.\n        \n        Capabilities: Advanced reasoning, coding, math,\
        \ general, multimodal, long context, thinking\n        Quality: Premium tier\
        \ with thinking capabilities\n        Cost: $1.25/M input tokens, $10.00/M\
        \ output tokens (\u2264200K), higher for >200K\n        Context: 1,048,576\
        \ tokens (~1M tokens)\n        Max Output: 65,535 tokens\n        \n     \
        \   Best for: Frontier reasoning tasks, deep analysis of long documents or\
        \ codebases, and complex multimodal applications requiring the highest quality.\n\
        \        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
- class_name: verl.tools.router_tool.RouterTool
  config: {}
  tool_schema:
    type: function
    function:
      name: call_gemini_2_5_flash_lite
      description: "Call Gemini 2.5 Flash Lite model. High-throughput Gemini model\
        \ optimized for cost-efficiency with an optional, controllable reasoning mode.\n\
        \        \n        Capabilities: Reasoning, coding, math, general, multimodal,\
        \ long context, high throughput\n        Quality: Budget tier optimized for\
        \ cost and throughput\n        Cost: $0.10/M input tokens, $0.40/M output\
        \ tokens\n        Context: 1,048,576 tokens (~1M tokens)\n        Max Output:\
        \ 65,535 tokens\n        \n        Best for: High-volume applications and\
        \ cost-sensitive projects requiring large context, where developers can dynamically\
        \ enable reasoning for higher quality on specific tasks.\n        "
      parameters:
        type: object
        properties:
          optimized_system_prompt:
            type: string
            description: Optimized system prompt for this specific model and task
          temperature:
            type: number
            description: Sampling temperature (0.0 to 2.0)
            default: 1.0
        required:
        - optimized_system_prompt
