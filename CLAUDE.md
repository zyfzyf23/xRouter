# CLAUDE.md - XRouter Lightweight Refactoring Guide
è¯·ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”é—®é¢˜ã€‚
åœ¨ç»™å‡ºä»£ç æ—¶ï¼Œè€ƒè™‘æ˜¯å¦æ€¥å‰§å ç”¨äº†å¤§é‡èµ„æºï¼Œæ¯”å¦‚ä¸å¯ä»¥çªç„¶å ç”¨å¤§é‡å†…å­˜ã€‚
é¡¹ç›®æ–‡ä»¶scriptsä¸‹çš„ä»£ç æ˜¯é‡æ„ä»£ç ï¼Œå¹¶ä¸æ˜¯xRouter çš„åŸç”Ÿä»£ç ï¼Œå¦‚æœä»»åŠ¡è¦æ±‚å¤ç°xRouterçš„ä»£ç ï¼Œå°½å¯èƒ½å¿½ç•¥scriptsæ–‡ä»¶å¤¹ä¸‹çš„ä»£ç ã€‚

**ğŸš¨ æ³¨æ„ï¼šå½“å‰é¡¹ç›®æ­£åœ¨è¿›è¡Œç‰¹å®šæ–¹å‘çš„é‡æ„ã€‚è¯·ä¼˜å…ˆéµå¾ªä»¥ä¸‹â€œå½“å‰å¼€å‘ä»»åŠ¡â€ä¸­çš„æŒ‡ç¤ºï¼Œå¿½ç•¥ä¸‹æ–¹å…³äº Ray é›†ç¾¤ã€vLLM å’Œåˆ†å¸ƒå¼è®­ç»ƒçš„æ—§æŒ‡ä»¤ã€‚**

## 1. ğŸ¯ å½“å‰å¼€å‘ä»»åŠ¡ï¼šè½»é‡åŒ–ç¦»çº¿ DPO é‡æ„

### é¡¹ç›®ç›®æ ‡
æˆ‘ä»¬è¦å°† XRouter ä»åŸæœ¬çš„â€œåœ¨çº¿å¼ºåŒ–å­¦ä¹  (DAPO)â€æ¶æ„ä¿®æ”¹ä¸ºé€‚åˆå•å¡ç¬”è®°æœ¬ (RTX 4060 8GB) è¿è¡Œçš„ **â€œç¦»çº¿ç¼“å­˜ + DPOâ€** æ¶æ„ã€‚

### ç¡¬ä»¶é™åˆ¶ (Hard Constraints)
- **GPU**: NVIDIA RTX 4060 Laptop (8GB VRAM)
- **Environment**: WSL2 (Ubuntu 22.04)
- **ç¦æ­¢é¡¹**: 
    - âŒ ç¦æ­¢ä½¿ç”¨ vLLM æˆ– SGLang (æ˜¾å­˜ä¸è¶³)ã€‚
    - âŒ ç¦æ­¢ä½¿ç”¨ Ray åˆ†å¸ƒå¼è®­ç»ƒã€‚
    - âŒ ç¦æ­¢åŠ è½½ 7B ä»¥ä¸Šçš„æœ¬åœ°æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚
- **å¿…é¡»é¡¹**:
    - âœ… è®­ç»ƒå¿…é¡»ä½¿ç”¨ `bitsandbytes` (4-bit Quantization)ã€‚
    - âœ… è®­ç»ƒå¿…é¡»ä½¿ç”¨ `peft` (LoRA)ã€‚
    - âœ… åŸºåº§æ¨¡å‹é”å®šä¸º `Qwen/Qwen2.5-1.5B-Instruct`ã€‚

### ç¯å¢ƒé™åˆ¶
- **condaç¯å¢ƒ**: py310

---

## 2. ğŸ—ºï¸ å¼€å‘è·¯çº¿å›¾ (Step-by-Step)

è¯·æŒ‰ä»¥ä¸‹äº”ä¸ªé˜¶æ®µååŠ©æˆ‘å®Œæˆä»£ç ç¼–å†™ã€‚

### é˜¶æ®µä¸€ï¼šæ ¸å¿ƒå®šä½ (Discovery)
- **ç›®æ ‡**: æ‰¾åˆ°â€œç¼–æ’å¼•æ“ (Orchestration Engine)â€é€»è¾‘ï¼Œå³è´Ÿè´£è°ƒç”¨ `litellm` æˆ–å¤–éƒ¨ API çš„æ ¸å¿ƒå‡½æ•°ã€‚
- **å…³é”®æ–‡ä»¶**: é‡ç‚¹å…³æ³¨ `verl/tools/utils/router_utils.py`ã€‚
- **ä»»åŠ¡**: ç†è§£ `call_model` æ¥å£ï¼Œå‡†å¤‡å°†å…¶å‰¥ç¦»å‡ºæ¥ç”¨äºé€ æ•°æ®ã€‚

### é˜¶æ®µäºŒï¼šæ„å»ºç¦»çº¿ç¼“å­˜ (Offline Data Gen)
- **è„šæœ¬ç›®æ ‡**: `scripts/generate_offline_cache.py`
- **é€»è¾‘**: 
    1. è¯»å–è®­ç»ƒæ•°æ®é›† (å¦‚ GSM8K)ã€‚
    2. **å¼ºåˆ¶éå† (Forced Traversal)**: ä¸ä½¿ç”¨ Router å†³ç­–ï¼Œè€Œæ˜¯å¯¹æ¯ä¸ªé—®é¢˜ï¼Œå¼ºåˆ¶è°ƒç”¨æ¨¡å‹æ± ä¸­çš„æ‰€æœ‰æ¨¡å‹ (å¦‚ `["gpt-4o", "qwen-turbo", "gpt-3.5"]`)ã€‚
    3. **æ•°æ®è®°å½•**: å¿…é¡»ä¿å­˜ `prompt`, `model_name`, `response`, `is_correct` (éœ€å¤ç°åŸè¯„ä¼°é€»è¾‘), `token_usage`ã€‚
- **æ³¨æ„**: ä»…è°ƒç”¨ API æˆ–è½»é‡çº§æ¨ç†ï¼Œä¸åŠ è½½ RL Actor æ¨¡å‹ã€‚

### é˜¶æ®µä¸‰ï¼šDPO æ•°æ®æ„å»º (Preprocessing)
- **è„šæœ¬ç›®æ ‡**: `scripts/preprocess_dpo.py`
- **é€»è¾‘**: 
    1. è¯»å– `offline_cache.jsonl`ã€‚
    2. **å¥–åŠ±å…¬å¼**: å®ç°è®ºæ–‡å…¬å¼ $R = R_{binary} \times (K - \lambda C)$ã€‚
       - è‹¥ `is_correct` ä¸º Falseï¼ŒReward = 0ã€‚
       - è‹¥ Correctï¼ŒReward = $1.0 - 0.1 \times Cost$ (ç¤ºä¾‹ç³»æ•°)ã€‚
    3. **é…å¯¹ç”Ÿæˆ**: å¯¹åŒä¸€ Promptï¼Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Rewardã€‚
       - `Reward(A) > Reward(B)` -> `chosen=A`, `rejected=B`ã€‚
    4. **è¾“å‡º**: HuggingFace Dataset æ ¼å¼ (`dpo_train_data.json`)ã€‚

### é˜¶æ®µå››ï¼šè½»é‡åŒ–è®­ç»ƒ (Lightweight Training)
- **è„šæœ¬ç›®æ ‡**: `scripts/train_dpo_light.py`
- **å·¥å…·æ ˆ**: ä½¿ç”¨ `trl` (DPOTrainer) + `peft` + `bitsandbytes`ã€‚
- **é…ç½®**:
    - Base Model: `Qwen/Qwen2.5-1.5B-Instruct`
    - Quantization: `load_in_4bit=True` (å…³é”®ï¼é˜²æ­¢ OOM)
    - LoRA: `r=16`, `target_modules=["q_proj", "v_proj", ...]`
    - Batch Size: 1 (é…åˆ Gradient Accumulation)

### é˜¶æ®µäº”ï¼šé›†æˆä¸éªŒè¯ (Evaluation)
- **è„šæœ¬ç›®æ ‡**: `scripts/evaluate_lora.py`
- **é€»è¾‘**:
    1. åŠ è½½ Base Model (1.5B) + è®­ç»ƒå¥½çš„ LoRA Adapterã€‚
    2. æ¢å¤ Router çš„è‡ªä¸»å†³ç­–æ¨¡å¼ (ä¸å†å¼ºåˆ¶éå†)ã€‚
    3. è¿è¡Œæµ‹è¯•é›†ï¼Œç»Ÿè®¡ **Accuracy** å’Œ **Total Cost**ã€‚

---

## 3. ğŸ› ï¸ å½“å‰ç¯å¢ƒé…ç½® (Current Env)

æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯è½»é‡åŒ–ç¯å¢ƒï¼Œä¸åŸæ–‡æ¡£ä¸åŒï¼š
```bash
python: 3.10
torch: 2.5.1+cu121
libraries: flash_attn (pre-compiled), bitsandbytes, peft, trl, litellm

### æ ¸å¿ƒæ¶æ„ç»„ä»¶

```
xRouter/
â”œâ”€â”€ verl/                           # VERL å¼ºåŒ–å­¦ä¹ æ¡†æ¶
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ utils/router_utils.py     # æ ¸å¿ƒè·¯ç”±å™¨åŠŸèƒ½å’Œæ¨¡å‹è§„èŒƒ
â”‚   â”‚   â”œâ”€â”€ router_tool.py           # è·¯ç”±å·¥å…·å®ç°
â”‚   â”‚   â””â”€â”€ schemas/                # å·¥å…·æ¨¡å¼å®šä¹‰
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ rollout/sglang_rollout/  # SGLang æ¨ç†åç«¯
â”‚   â”‚   â””â”€â”€ reward_manager/         # æˆæœ¬æ„ŸçŸ¥å¥–åŠ±å¡‘é€ 
â”‚   â””â”€â”€ recipe/dapo/                # DAPO è®­ç»ƒç®—æ³•
â”œâ”€â”€ data_preprocess/
â”‚   â””â”€â”€ router_data_preprocess.py    # è®­ç»ƒæ•°æ®ç”Ÿæˆç®¡é“
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ sglang_multiturn/config/tool_config/
â”‚       â””â”€â”€ router_tool_config.yaml # 20+ æ¨¡å‹å·¥å…·å®šä¹‰
â”œâ”€â”€ train/                          # è®­ç»ƒè„šæœ¬å’Œé…ç½®
â”œâ”€â”€ evaluation/                     # è¯„ä¼°å’ŒæœåŠ¡éƒ¨ç½²
â””â”€â”€ tests/router/                    # è·¯ç”±å™¨å•å…ƒæµ‹è¯•
```

## å¸¸ç”¨å¼€å‘å‘½ä»¤

### ç¯å¢ƒè®¾ç½®
```bash
# åˆ›å»ºåŸºç¡€ç¯å¢ƒ
conda create -n xrouter python=3.12
conda activate xrouter

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install uv
uv pip install torch==2.6.0
uv pip install flash-attn==2.7.3 --no-build-isolation
uv pip install -e .[gpu,math,vllm,test]

# è·¯ç”±å™¨ç‰¹å®šä¾èµ–
pip install litellm rich python-dotenv

# API å¯†é’¥é…ç½®ï¼ˆè‡³å°‘éœ€è¦ä¸€ä¸ªï¼‰
export OPENAI_API_KEY="your_openai_key"
export TOGETHER_API_KEY="your_together_key"
export GEMINI_API_KEY="your_gemini_key"
```

### æµ‹è¯•å’ŒéªŒè¯
```bash
# æµ‹è¯•æ¨¡å‹è¿æ¥
python tests/router/test_simple_connection.py

# éªŒè¯æ‰€æœ‰æ¨¡å‹å¯è®¿é—®æ€§
python -c "from verl.tools.utils.router_utils import MODEL_SPECS; print(f'{len(MODEL_SPECS)} models available')"

# å•å…ƒæµ‹è¯•
pytest tests/router/ -v
```

### æ•°æ®é¢„å¤„ç†
```bash
# ä¸‹è½½åŸºç¡€è®­ç»ƒæ•°æ®
python scripts/tools/download_guru.py

# ç”Ÿæˆè·¯ç”±å™¨è®­ç»ƒæ•°æ®ï¼ˆå›°éš¾ä»»åŠ¡ï¼‰
python data_preprocess/router_data_preprocess.py \
    --use_fixed_sets \
    --fixed_set_1_percentage 0.5 \
    --fixed_set_2_percentage 0.1 \
    --fixed_set_3_percentage 0.05 \
    --num_repetitions 2 \
    --premium_min 1 --premium_max 5 \
    --budget_min 1 --budget_max 5 \
    --standard_min 1 --standard_max 5 \
    --specialized_min 0 --specialized_max 3 \
    --seed 42 \
    --max_system_prompt_length 2000 \
    --output_dir data/train_hard_MMDD \
    --input_dir data/train_filter_015 \
    --max_num_samples 400
```

### è®­ç»ƒé…ç½®å’Œå¯åŠ¨
```bash
# è®¾ç½® Ray é›†ç¾¤
export RAY_TMPDIR=$HOME/ray_tmp
ray stop || true
head_node_ip=$(hostname -I | awk '{print $1}')
ray start --head --node-ip-address="$head_node_ip" --port=6595 --include-dashboard=False --block &

# å¯åŠ¨è®­ç»ƒ
bash train/example_singlenode_router1.sh
```

å…³é”®è®­ç»ƒå‚æ•°ï¼š
- `BASE_MODEL`: åŸºç¡€æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ Qwen/Qwen2.5-7B-Instructï¼‰
- `TRAIN_DATA_DIR`: é¢„å¤„ç†çš„è·¯ç”±å™¨è®­ç»ƒæ•°æ®
- `reward_lambda`: æˆæœ¬æƒ©ç½šç³»æ•°ï¼ˆé»˜è®¤ 2.0ï¼‰
- `tool_config_path`: è·¯ç”±å™¨å·¥å…·é…ç½®è·¯å¾„
- `max_turns`: æœ€å¤§æ™ºèƒ½ä½“è½®æ¬¡ï¼ˆé»˜è®¤ 3ï¼‰

### éƒ¨ç½²å’ŒæœåŠ¡
```bash
# å¯åŠ¨è·¯ç”±å™¨æ¨¡å‹æœåŠ¡å™¨
cd evaluation
bash host_router.sh  # ç«¯å£ 8000

# å¯åŠ¨ OpenAI å…¼å®¹ API
bash serve_router.sh  # ç«¯å£ 8800

# è¿è¡ŒåŸºå‡†æµ‹è¯•
python benchmark_router.py \
    --eval_data_dir data/offline_eval/ \
    --output_dir evaluation/outputs/
```

## è·¯ç”±ç³»ç»Ÿæ¶æ„

### æ¨¡å‹åˆ†å±‚å’Œé€‰æ‹©ç­–ç•¥

**é«˜çº§æ¨¡å‹**ï¼šGPT-5ã€GPT-4.1ã€o3ã€Qwen3-235B-Instructã€Kimi K2
- ç”¨äºå…³é”®ä»»åŠ¡ã€å¤æ‚æ™ºèƒ½ä½“å·¥ä½œæµ

**æ ‡å‡†æ¨¡å‹**ï¼šGPT-5-Miniã€GPT-4.1-Miniã€o4-Miniã€GPT-OSS-120B
- ç”¨äºæˆæœ¬æ•æ„Ÿå·¥ä½œæµã€æ€§èƒ½å¹³è¡¡åº”ç”¨

**é¢„ç®—æ¨¡å‹**ï¼šGPT-5-Nanoã€GPT-4.1-Nanoã€GPT-4o-Miniã€GPT-OSS-20B
- ç”¨äºå¤§é‡åº”ç”¨ã€å®æ—¶äº¤äº’

**ä¸“ä¸šæ¨¡å‹**ï¼šo3ã€DeepSeek-R1ã€Qwen3-235B-Thinkingã€Qwen3-Coder-480B
- ç”¨äºæ•°å­¦æ¨ç†ã€ç§‘å­¦ç ”ç©¶ã€ç¼–ç¨‹ä»»åŠ¡

### DAPO è®­ç»ƒç®—æ³•

- **åˆ†å¸ƒå¼ä¼˜åŠ¿ä¼°è®¡**ï¼šä½¿ç”¨å¤šä¸ªæ¨ä¼°è®¡ä¼˜åŠ¿åˆ†å¸ƒ
- **æˆæœ¬æ„ŸçŸ¥å¥–åŠ±**ï¼š`reward = quality - Î» Ã— cost` å½¢å¼çš„å¥–åŠ±å‡½æ•°
- **å¤šè½®ä¿¡ç”¨åˆ†é…**ï¼šè·¨æ™ºèƒ½ä½“è½®æ¬¡çš„æ­£ç¡®å¥–åŠ±å½’å±
- **å·¥å…·ä½¿ç”¨å­¦ä¹ **ï¼šè·¯ç”±å™¨å­¦ä¹ æœ€ä¼˜çš„ `call_<model_name>` å’Œ `select_response` å·¥å…·ä½¿ç”¨æ¨¡å¼

### è·¯ç”±ç­–ç•¥

**ç®€å•æ¨¡å¼**ï¼šæ¯è½®åªé€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼ˆæœ€å°å»¶è¿Ÿï¼‰
```bash
python data_preprocess/router_data_preprocess.py --simple_mode
```

**æ™ºèƒ½ä½“æ¨¡å¼**ï¼šå¯è°ƒç”¨å¤šä¸ªæ¨¡å‹å¹¶ä½¿ç”¨ `select_response` è¿›è¡Œé›†æˆå†³ç­–ï¼ˆé»˜è®¤ï¼‰
```bash
# åœ¨æ•°æ®é¢„å¤„ç†ä¸­é»˜è®¤å¯ç”¨
python data_preprocess/router_data_preprocess.py  # é»˜è®¤ä¸ºæ™ºèƒ½ä½“æ¨¡å¼
```

## å…³é”®é…ç½®æ–‡ä»¶

### å·¥å…·é…ç½®
- `examples/sglang_multiturn/config/tool_config/router_tool_config.yaml`ï¼šå®šä¹‰ 20+ ä¸ªæ¨¡å‹å·¥å…·
- æ¯ä¸ªå·¥å…·åŒ…å«æ¨¡å‹æè¿°ã€å‚æ•°å’Œæˆæœ¬ä¿¡æ¯
- æ”¯æŒ `call_<model_name>` å‡½æ•°å’Œ `select_response` é€‰æ‹©æœºåˆ¶

### è®­ç»ƒé…ç½®
- `train/example_singlenode_router1.sh`ï¼šä¸»è¦è®­ç»ƒè„šæœ¬
- `data_preprocess/router_data_preprocess.py`ï¼šè®­ç»ƒæ•°æ®ç”Ÿæˆç®¡é“
- `verl/tools/utils/router_utils.py`ï¼šæ¨¡å‹è§„èŒƒå’Œç»Ÿä¸€è·¯ç”±å™¨æ¥å£

## æ¨¡å‹è§„èŒƒå’Œ API é›†æˆ

æ ¸å¿ƒæ¨¡å‹æ³¨å†Œè¡¨åœ¨ `verl/tools/utils/router_utils.py` ä¸­å®šä¹‰ï¼ŒåŒ…å« 20+ ä¸ªæ¨¡å‹ï¼š

```python
# ä½¿ç”¨ç¤ºä¾‹
from verl.tools.utils.router_utils import LLMRouter, MODEL_SPECS

router = LLMRouter()
response, metadata = router.call_model(
    "gpt-4o",
    [{"role": "user", "content": "è§£é‡Šé‡å­è®¡ç®—"}],
    {"temperature": 0.7, "max_tokens": 1024}
)
```

æ¯ä¸ªæ¨¡å‹è§„èŒƒåŒ…å«ï¼š
- å®šä»·ä¿¡æ¯ï¼ˆè¾“å…¥/è¾“å‡ºæ¯ç™¾ä¸‡ token ç¾å…ƒï¼‰
- ä¸Šä¸‹æ–‡çª—å£å’Œæœ€å¤§è¾“å‡ºé•¿åº¦
- èƒ½åŠ›æ ‡ç­¾ï¼ˆæ¨ç†ã€ç¼–ç¨‹ã€æ•°å­¦ç­‰ï¼‰
- æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ

## æ¨ç†åç«¯æ”¯æŒ

### SGLang é›†æˆ
```bash
# SGLang åç«¯ç”¨äºè®­ç»ƒæ—¶æ¨ç†
export SGLANG_HTTP_PORT="${SGLANG_HTTP_PORT:-30000}"
python -m verl.third_party.sglang.srt_runner \
    --model-path $model_path \
    --port $SGLANG_HTTP_PORT
```

### vLLM é›†æˆ
```bash
# vLLM åç«¯ç”¨äºç”Ÿäº§éƒ¨ç½²
python -m verl.third_party.vllm.vllm_v_0_6_3.llm_engine \
    --model $model_path \
    --gpu-memory-utilization 0.6
```

## æˆæœ¬æ„ŸçŸ¥è®­ç»ƒ

### å¥–åŠ±å‡½æ•°è®¾è®¡
- `reward_lambda` æ§åˆ¶æˆæœ¬æ•æ„Ÿæ€§ï¼ˆè¶Šé«˜ = è¶Šå¼ºçš„æˆæœ¬æƒ©ç½šï¼‰
- `reward_K` è®¾ç½®å¥–åŠ±é˜ˆå€¼
- `cost_max` æœ€å¤§å½’ä¸€åŒ–æˆæœ¬
- æ”¯æŒå¤šè½®ä¿¡ç”¨åˆ†é…å’Œå·¥å…·ä½¿ç”¨è·Ÿè¸ª

### è¯¾ç¨‹å­¦ä¹ 
- **å›ºå®šæ¨¡å‹é›†**ï¼šä¸‰ä¸ªé¢„å®šä¹‰æ¨¡å‹é›†ç”¨äºæ¸è¿›éš¾åº¦è®­ç»ƒ
- **åŠ¨æ€æ¨¡å‹æ± **ï¼šæ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«å”¯ä¸€çš„æ¨¡å‹ç»„åˆ
- **æç¤ºä¼˜åŒ–**ï¼šè·¯ç”±å™¨å­¦ä¹ ä¸ºæ¯ä¸ªç›®æ ‡æ¨¡å‹è®¾è®¡æœ€ä¼˜ç³»ç»Ÿæç¤º

## API ä½¿ç”¨ç¤ºä¾‹

### OpenAI å…¼å®¹å®¢æˆ·ç«¯
```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8800/v1",
    api_key="dummy"  # æœ¬åœ°éƒ¨ç½²ä¸éœ€è¦ API å¯†é’¥
)

response = client.chat.completions.create(
    model="router-tool-rl",
    messages=[
        {"role": "user", "content": "ç¼–å†™ä¸€ä¸ª Python å‡½æ•°æ¥åè½¬é“¾è¡¨"}
    ],
    max_tokens=1000
)

# è®¿é—®è·¯ç”±å…ƒæ•°æ®
metadata = response.router_metadata
print(f"ä½¿ç”¨æ¨¡å‹: {metadata['model_used']}")
print(f"æ€»æˆæœ¬: ${metadata['total_cost']:.6f}")
print(f"è·¯ç”±ç­–ç•¥: {metadata['routing_strategy']}")
```

### ç›´æ¥è·¯ç”±å™¨ä½¿ç”¨
```python
from verl.tools.utils.router_utils import LLMRouter

router = LLMRouter()

# è°ƒç”¨ç‰¹å®šæ¨¡å‹
response, metadata = await router.acall_model(
    model_id="gpt-5-mini",
    messages=[{"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ "}],
    sampling_params={"temperature": 0.7, "max_tokens": 1024}
)

print(f"å“åº”: {response}")
print(f"æˆæœ¬: ${metadata['cost']:.6f}")
print(f"Token: {metadata['input_tokens']} + {metadata['output_tokens']}")
```

## è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•

### ç¦»çº¿è¯„ä¼°
```bash
# åœ¨ 17 ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°
python evaluation/benchmark_router.py \
    --model_path /path/to/your/trained/router \
    --eval_data_dir data/offline_eval/ \
    --output_dir ./evaluation_results
```

### åœ¨çº¿è¯„ä¼°
```bash
# è¿è¡Œå…¨é¢æµ‹è¯•å¥—ä»¶
python evaluation/test_serve.py --test all

# ç‰¹å®šæµ‹è¯•ç±»åˆ«
python evaluation/test_serve.py --test math_problem
python evaluation/test_serve.py --test coding_task
python evaluation/test_serve.py --test reasoning_task
```

## å¼€å‘æ³¨æ„äº‹é¡¹

- æ‰€æœ‰è·¯ç”±å™¨å·¥å…·éµå¾ª `call_<model_name>` å‘½åçº¦å®š
- æ¨¡å‹è§„èŒƒåœ¨ `MODEL_SPECS` ä¸­æ˜¯å•ä¸€çœŸå®æ¥æº
- è®­ç»ƒä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è¿›è¡Œæ¸è¿›æ¨¡å‹æ± éš¾åº¦
- ç³»ç»Ÿæ”¯æŒç®€å•è·¯ç”±ï¼ˆå•æ¨¡å‹ï¼‰å’Œæ™ºèƒ½ä½“æ¨¡å¼ï¼ˆå¤šæ¨¡å‹ + é€‰æ‹©ï¼‰
- æˆæœ¬è·Ÿè¸ªé›†æˆåœ¨è®­ç»ƒå’Œæ¨ç†ç®¡é“ä¸­
- æ€ç»´æ¨¡å‹ï¼ˆo3ã€o4ã€DeepSeek-R1ï¼‰éœ€è¦é«˜ token é™åˆ¶ï¼ˆ8192+ï¼‰ä»¥æ”¯æŒå†…éƒ¨æ¨ç†
- ä½¿ç”¨ FSDP å’Œå‚æ•°/ä¼˜åŒ–å™¨å¸è½½è¿›è¡Œå†…å­˜æ•ˆç‡è®­ç»ƒ